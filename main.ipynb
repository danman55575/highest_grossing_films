{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment: Data Wrangling, Visualization, and Web Presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import all the necessary libraries: requests and matplotlib for parsing data, sqlite3 for interaction with the SQLite local database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Dict\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import sqlite3\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extracting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_URL = \"https://en.wikipedia.org/wiki/List_of_highest-grossing_films\"\n",
    "WIKIPEDIA_MAIN_URL = \"https://en.wikipedia.org\"\n",
    "USER_AGENT = \"Mozilla/5.0\"\n",
    "\n",
    "class Film:\n",
    "    def __init__(\n",
    "            self,\n",
    "            title: str,\n",
    "            release_year: Optional[int] = None,\n",
    "            director: Optional[str] = None,\n",
    "            box_office: Optional[str] = None,\n",
    "            country: Optional[str] = None,\n",
    "            ):\n",
    "        self.title = title\n",
    "        self.release_year = release_year\n",
    "        self.director = director\n",
    "        self.box_office = box_office\n",
    "        self.country = country\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"\"\"Film(title={self.title},\n",
    "            release_year={self.release_year},\n",
    "            director={self.director},\n",
    "            box_office={self.box_office},\n",
    "            country={self.country})\"\"\"\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"title\": self.title,\n",
    "            \"release_year\": self.release_year,\n",
    "            \"director\": self.director,\n",
    "            \"box_office\": self.box_office,\n",
    "            \"country\": self.country\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_text(raw_text: str) -> List[str]:\n",
    "    pattern = r'[A-Za-z. ]{2,}'\n",
    "    processed_text = ', '.join(re.findall(pattern, raw_text))\n",
    "    return processed_text\n",
    "\n",
    "def parse_film_details(film: Film, film_wikipedia_url: str):\n",
    "    film_page = requests.get(film_wikipedia_url, headers={'User-Agent': USER_AGENT})\n",
    "    film_page_soup = BeautifulSoup(film_page.content, 'html.parser')\n",
    "    film_table = film_page_soup.select_one('table.infobox.vevent')\n",
    "    # parse the directors of the movie\n",
    "    rows = film_table.find_all('tr')\n",
    "    for row in rows:\n",
    "        title = row.select_one('th')\n",
    "        if not title:\n",
    "            continue\n",
    "        if title.get_text().strip() == \"Directed by\":\n",
    "            raw_directors_text = row.select_one('td').get_text(separator=',').strip()\n",
    "            film.director = get_processed_text(raw_directors_text)\n",
    "        if title.get_text().strip() in ['Country', 'Countries']:\n",
    "            raw_country_text = row.select_one('td').get_text().strip()\n",
    "            film.country = get_processed_text(raw_country_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_grossing_films():\n",
    "    main_page = requests.get(MAIN_URL, headers={'User-Agent': USER_AGENT})\n",
    "    main_page_soup = BeautifulSoup(main_page.content, 'html.parser')\n",
    "    table = main_page_soup.select_one('table.wikitable.sortable')\n",
    "\n",
    "    if not table:\n",
    "        raise ValueError(\"Main table not found\")\n",
    "    \n",
    "    films = []\n",
    "    for row in table.select('tbody tr')[1:]:\n",
    "        attributes = row.find_all('td')\n",
    "        gross_raw_text = attributes[2].get_text().strip()\n",
    "        release_year = int(attributes[3].get_text().strip())\n",
    "\n",
    "        # get cleaned gross information\n",
    "        pattern = r'[0-9,]{2,}'\n",
    "        gross = '$' + re.findall(pattern, gross_raw_text)[0]\n",
    "\n",
    "        title = row.select_one('th a').get_text().strip()\n",
    "        wikipedia_url = WIKIPEDIA_MAIN_URL + row.select_one('th a')['href']\n",
    "        film = Film(title, release_year=release_year, box_office=gross)\n",
    "        parse_film_details(film, wikipedia_url)\n",
    "        films.append(film)\n",
    "\n",
    "    return films\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_db(database_name: str):\n",
    "    try:\n",
    "        conn = sqlite3.connect(database_name)\n",
    "        return conn\n",
    "    except sqlite3.Error as error:\n",
    "        print(f\"Error with database connection: {error}\")\n",
    "        return None\n",
    "\n",
    "def create_table(conn: sqlite3.Connection):\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS films (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                title TEXT NOT NULL,\n",
    "                release_year INTEGER,\n",
    "                director TEXT NOT NULL,\n",
    "                box_office TEXT,\n",
    "                country TEXT\n",
    "            )\n",
    "            ''')\n",
    "        conn.commit()\n",
    "    except sqlite3.Error as error:\n",
    "        print(f\"Failed to create table: {error}\")\n",
    "\n",
    "def insert_films_to_db(conn: sqlite3.Connection, films: List[Film]):\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        for film in films:\n",
    "            cursor.execute(f\"INSERT INTO films (title, release_year, director, box_office, country) VALUES (?, ?, ?, ?, ?)\", (\n",
    "                film.title,\n",
    "                film.release_year,\n",
    "                film.director,\n",
    "                film.box_office,\n",
    "                film.country\n",
    "            ))\n",
    "        conn.commit()\n",
    "    except sqlite3.Error as error:\n",
    "        print(f\"Failed to insert values to database: {error}\")\n",
    "\n",
    "def get_films_from_db(conn: sqlite3.Connection) -> List[Film]:\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SELECT * FROM films\")\n",
    "        films = [Film(*film[1:]) for film in cursor.fetchall()]\n",
    "        return films\n",
    "    except sqlite3.Error as error:\n",
    "        print(f\"Failed to get all the films from database: {error}\")\n",
    "\n",
    "def delete_films_from_db(conn: sqlite3.Connection) -> List[Film]:\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"DELETE FROM films\")\n",
    "        conn.commit()\n",
    "    except sqlite3.Error as error:\n",
    "        print(f\"Failed to delete all films from the database: {error}\")\n",
    "\n",
    "def close_connection(conn: sqlite3.Connection):\n",
    "    if conn:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_films_into_json(json_path: str, films: List[Film]):\n",
    "    films_dict = [film.to_dict() for film in films]\n",
    "    films_dict.sort(key=lambda film: film['box_office'])\n",
    "    films_dict = films_dict[::-1]\n",
    "    for rank, film in enumerate(films_dict):\n",
    "        film['rank'] = rank+1\n",
    "    with open(json_path, \"w\", encoding='utf-8') as file:\n",
    "        json.dump(films_dict, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "films = parse_grossing_films()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = connect_to_db(\"grossing_films.db\")\n",
    "create_table(connection)\n",
    "delete_films_from_db(connection)\n",
    "insert_films_to_db(connection, films)\n",
    "films = get_films_from_db(connection)\n",
    "close_connection(connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_films_into_json(\"movies.json\", films)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
